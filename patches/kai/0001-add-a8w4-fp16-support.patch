From b37f61d5e827b115b29b688b17ffa6b5f6c72b2e Mon Sep 17 00:00:00 2001
From: Tianyu Li <tianyu.li@arm.com>
Date: Wed, 18 Dec 2024 18:09:27 +0800
Subject: [PATCH] add a8w4 fp16 input and output support

Add fp16 input support for lhs packing for 32 groupwise int8 packing.
Add fp16 output support for matmul a8w4 32 groupwise gemv kernel.
Add fp16 output support for matmal a8w4 32 groupwise gemm kernel.

Signed-off-by: Tianyu Li <tianyu.li@arm.com>
---
 CMakeLists.txt                                |   3 +
 kai/ukernels/matmul/BUILD.bazel               |   3 +
 ...8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c | 221 ++++++
 ...8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h | 143 ++++
 ...6_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c | 701 ++++++++++++++++++
 ...6_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h | 144 ++++
 ...ul_clamp_f16_qsi8d32p_qsi4c32p_interface.h |  55 ++
 .../pack/kai_lhs_quant_pack_qsi8d32p_f16.c    | 124 ++++
 .../pack/kai_lhs_quant_pack_qsi8d32p_f16.h    |  84 +++
 14 files changed, 2539 insertions(+), 5 deletions(-)
 create mode 100644 kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c
 create mode 100644 kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h
 create mode 100644 kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c
 create mode 100644 kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h
 create mode 100644 kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_interface.h
 create mode 100644 kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.c
 create mode 100644 kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.h

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 3a1cef6..338aa49 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -82,6 +82,7 @@ endif()
 set(KLEIDIAI_FILES_SCALAR
     kai/ukernels/matmul/pack/kai_lhs_quant_pack_qai8dxp_f32.c
     kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32.c
+    kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.c
     kai/ukernels/matmul/pack/kai_rhs_pack_kxn_qsi4c32p_qsu4c32s1s0.c
     kai/ukernels/matmul/pack/kai_rhs_pack_kxn_qsi4cxp_qs4cxs1s0.c
     kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4cxp_qs4cxs1s0.c
@@ -120,6 +121,7 @@ set(KLEIDIAI_FILES_NEON
 )

 set(KLEIDIAI_FILES_NEON_DOTPROD
+    kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp1x8_qsi4cxp4x8_1x4x32_neon_dotprod.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp1x8_qsi4cxp8x8_1x8x32_neon_dotprod.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp4x8_qsi4cxp4x4_16x4x32_neon_dotprod.c
@@ -135,6 +137,7 @@ set(KLEIDIAI_FILES_NEON_DOTPROD
 )

 set(KLEIDIAI_FILES_NEON_I8MM
+    kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp4x8_qsi4cxp4x8_4x4x32_neon_i8mm.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp4x8_qsi4cxp4x8_8x4x32_neon_i8mm.c
     kai/ukernels/matmul/matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp4x8_qsi4cxp8x8_4x8x32_neon_i8mm.c
diff --git a/kai/ukernels/matmul/BUILD.bazel b/kai/ukernels/matmul/BUILD.bazel
index e6b7f59..35ae3e8 100644
--- a/kai/ukernels/matmul/BUILD.bazel
+++ b/kai/ukernels/matmul/BUILD.bazel
@@ -21,6 +21,7 @@ package(default_visibility = ["//visibility:private"])
 # buildifier: keep sorted
 SCALAR_KERNELS = [
     "pack/kai_lhs_quant_pack_qai8dxp_f32",
+    "pack/kai_lhs_quant_pack_qsi8d32p_f16",
     "pack/kai_lhs_quant_pack_qsi8d32p_f32",
     "pack/kai_rhs_pack_kxn_qsi4cxp_qs4cxs1s0",
     "pack/kai_rhs_pack_nxk_qsi4cxp_qs4cxs1s0",
@@ -64,6 +65,7 @@ FP16_BF16_KERNELS = [

 # buildifier: keep sorted
 DOTPROD_KERNELS = [
+    "matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod",
     "matmul_clamp_f32_qai8dxp_qsi4c32p/kai_matmul_clamp_f32_qai8dxp1x8_qsi4c32p4x8_1x4x32_neon_dotprod",
     "matmul_clamp_f32_qai8dxp_qsi4c32p/kai_matmul_clamp_f32_qai8dxp1x8_qsi4c32p8x8_1x8x32_neon_dotprod",
     "matmul_clamp_f32_qai8dxp_qsi4cxp/kai_matmul_clamp_f32_qai8dxp1x8_qsi4cxp4x8_1x4x32_neon_dotprod",
@@ -80,6 +82,7 @@ DOTPROD_KERNELS = [

 # buildifier: keep sorted
 I8MM_KERNELS = [
+    "matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm",
     "matmul_clamp_f32_qai8dxp_qsi4c32p/kai_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm",
     "matmul_clamp_f32_qai8dxp_qsi4c32p/kai_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_8x4x32_neon_i8mm",
     "matmul_clamp_f32_qai8dxp_qsi4c32p/kai_matmul_clamp_f32_qai8dxp4x8_qsi4c32p8x8_4x8x32_neon_i8mm",
diff --git a/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c
new file mode 100644
index 0000000..3b08bae
--- /dev/null
+++ b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c
@@ -0,0 +1,221 @@
+
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#if !defined(__ARM_FEATURE_DOTPROD)
+#error "Dotprod extension required to compile this micro-kernel"
+#else
+#include "kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h"
+
+#include <arm_neon.h>
+#include <stdint.h>
+
+#include "kai/kai_common.h"
+
+static const size_t kai_m_step = 1;
+static const size_t kai_n_step = 4;
+static const size_t kai_mr = 1;
+static const size_t kai_nr = 4;
+static const size_t kai_kr = 16;
+static const size_t kai_sr = 2;
+static const size_t kai_bl = 32;
+static const size_t kai_num_bytes_multiplier = sizeof(uint16_t);
+
+inline static size_t kai_num_bytes_per_block_lhs(void) {
+    return kai_bl * sizeof(int8_t) + kai_num_bytes_multiplier;
+}
+
+inline static size_t kai_num_bytes_per_block_rhs(void) {
+    return (kai_bl / 2) * sizeof(int8_t) + kai_num_bytes_multiplier;
+}
+
+inline static size_t kai_num_blocks_per_row(size_t k) {
+    KAI_ASSUME((k % kai_bl) == 0);
+    return k / kai_bl;
+}
+
+inline static size_t kai_lhs_packed_stride(size_t k) {
+    return kai_mr * kai_num_blocks_per_row(k) * kai_num_bytes_per_block_lhs();
+}
+
+inline static size_t kai_rhs_packed_stride(size_t k) {
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % kai_bl) == 0);
+
+    const size_t num_blocks_per_row = kai_num_blocks_per_row(k);
+    const size_t num_bytes_per_block = kai_num_bytes_per_block_rhs();
+
+    return kai_nr * (num_bytes_per_block * num_blocks_per_row);
+}
+
+size_t kai_get_m_step_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_m_step;
+}
+
+size_t kai_get_n_step_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_n_step;
+}
+
+size_t kai_get_mr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_mr;
+}
+
+size_t kai_get_nr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_nr;
+}
+
+size_t kai_get_kr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_kr;
+}
+
+size_t kai_get_sr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void) {
+    return kai_sr;
+}
+
+size_t kai_get_lhs_packed_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m_idx, size_t k, size_t bl) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+    KAI_ASSUME((m_idx % kai_m_step) == 0);
+
+    return (m_idx / kai_m_step) * kai_lhs_packed_stride(k);
+}
+
+size_t kai_get_rhs_packed_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t n_idx, size_t k, size_t bl) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+    KAI_ASSUME((n_idx % kai_n_step) == 0);
+
+    return (n_idx / kai_n_step) * kai_rhs_packed_stride(k);
+}
+
+size_t kai_get_dst_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m_idx, size_t n_idx, size_t dst_stride) {
+    KAI_ASSUME((m_idx % kai_m_step) == 0);
+    KAI_ASSUME((n_idx % kai_n_step) == 0);
+
+    return (n_idx * sizeof(float16_t)) + m_idx * dst_stride;
+}
+
+size_t kai_get_dst_size_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(size_t m, size_t n) {
+    return m * n * sizeof(float16_t);
+}
+
+void kai_run_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m, size_t n, size_t k, size_t bl, const void* lhs_packed, const void* rhs_packed,
+    float16_t* dst,  // NOLINT(readability-non-const-parameter)
+    size_t dst_stride_row, size_t dst_stride_col, float scalar_min, float scalar_max) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME(k % kai_bl == 0);
+    KAI_ASSUME(dst_stride_col == sizeof(float16_t));
+
+    if (m == 0) {
+        return;
+    }
+
+    const size_t num_blocks = k / kai_bl;
+    float clamp_vals[2] = {scalar_min, scalar_max};
+
+    __asm__ __volatile__(
+        "mov x26, #0x22\n"
+        "movi v1.16b, #0xf0\n"
+        "mov x25, %x[m]\n"
+        "mul x26, %x[num_blocks], x26\n"
+        "1:"  // Row loop
+        "mov x24, %x[rhs_packed]\n"
+        "mov x23, %x[n]\n"
+        "add x22, %x[dst], %x[dst_stride_row]\n"
+        "2:"  // Column loop
+        "mov x21, %x[lhs_packed]\n"
+        "movi v0.16b, #0x0\n"
+        "mov x20, %x[num_blocks]\n"
+        "3:"  // Block loop
+        "ldr d16, [x24, #0x0]\n"
+        "ld1r { v31.8h }, [x21]\n"
+        "add x24, x24, #0x8\n"
+        "add x21, x21, #0x2\n"
+        "ldr q30, [x24, #0x0]\n"
+        "ldr q29, [x24, #0x10]\n"
+        "movi v28.4s, #0x0\n"
+        "movi v27.4s, #0x0\n"
+        "ld1r { v26.2d }, [x21], #0x8\n"
+        "ldr q25, [x24, #0x20]\n"
+        "sub x20, x20, #0x1\n"
+        "ldr q24, [x24, #0x30]\n"
+        "fcvtl v31.4s, v31.4h\n"
+        "fcvtl v23.4s, v16.4h\n"
+        "add x24, x24, #0x40\n"
+        "ld1r { v22.2d }, [x21], #0x8\n"
+        "shl v21.16b, v30.16b, #0x4\n"
+        "shl v20.16b, v29.16b, #0x4\n"
+        "ld1r { v19.2d }, [x21], #0x8\n"
+        "ld1r { v18.2d }, [x21], #0x8\n"
+        "shl v17.16b, v25.16b, #0x4\n"
+        "and v30.16b, v30.16b, v1.16b\n"
+        "shl v16.16b, v24.16b, #0x4\n"
+        "and v29.16b, v29.16b, v1.16b\n"
+        ".inst 0x4e9a96bc  // sdot v28.4s, v21.16b, v26.16b\n"
+        ".inst 0x4e9a969b  // sdot v27.4s, v20.16b, v26.16b\n"
+        "and v25.16b, v25.16b, v1.16b\n"
+        "and v24.16b, v24.16b, v1.16b\n"
+        "fmul v23.4s, v23.4s, v31.4s\n"
+        ".inst 0x4e96963c  // sdot v28.4s, v17.16b, v22.16b\n"
+        ".inst 0x4e96961b  // sdot v27.4s, v16.16b, v22.16b\n"
+        ".inst 0x4e9397dc  // sdot v28.4s, v30.16b, v19.16b\n"
+        ".inst 0x4e9397bb  // sdot v27.4s, v29.16b, v19.16b\n"
+        ".inst 0x4e92973c  // sdot v28.4s, v25.16b, v18.16b\n"
+        ".inst 0x4e92971b  // sdot v27.4s, v24.16b, v18.16b\n"
+        "addp v28.4s, v28.4s, v27.4s\n"
+        "scvtf v28.4s, v28.4s, #0x4\n"
+        "fmla v0.4s, v28.4s, v23.4s\n"
+        "cbnz x20, 3b\n"
+        "ld1r { v17.4s }, [%x[clamp_vals]]\n"
+        "add x20, %x[clamp_vals], #0x4\n"
+        "cmp x23, #0x4\n"
+        "ld1r { v16.4s }, [x20]\n"
+        "fmax v0.4s, v0.4s, v17.4s\n"
+        "fmin v0.4s, v0.4s, v16.4s\n"
+        "fcvtn v0.4h, v0.4s\n"
+        "blt 4f\n"
+        "str d0, [%x[dst], #0x0]\n"
+        "b 7f\n"
+        "4:"  // Partial output
+        "mov x20, %x[dst]\n"
+        "tbz x23, #1, 5f\n"           // Check if at least 2 elements (bit 1 set in x23)
+        "st1 { v0.h }[0], [x20], #0x2\n"  // Store the first element and increment by 2 bytes
+        "tbz x23, #0, 6f\n"           // Check if exactly 2 elements (bit 0 cleared in x23)
+        "st1 { v0.h }[1], [x20], #0x2\n"  // Store the second element and increment
+        "st1 { v0.h }[2], [x20]\n"    // Store the third element (no increment needed)
+        "b 6f\n"
+
+        "5:"  // Case for only 1 element
+        "st1 { v0.h }[0], [x20]\n"
+        "b 6f\n"
+
+        "6:"  // Done
+        "tbz x23, #1, 7f\n"          // Check if at least 3 elements
+        "st1 { v0.h }[2], [x20]\n"   // Store the third element
+        "7:"  // Done
+        "subs x23, x23, #0x4\n"
+        "add %x[dst], %x[dst], #0x8\n"
+        "bgt 2b\n"
+        "subs x25, x25, #0x1\n"
+        "add %x[lhs_packed], %x[lhs_packed], x26\n"
+        "mov %x[dst], x22\n"
+        "bgt 1b\n"
+        : [dst] "+&r"(dst), [lhs_packed] "+&r"(lhs_packed)
+        : [clamp_vals] "r"(clamp_vals), [dst_stride_row] "r"(dst_stride_row), [m] "r"(m), [n] "r"(n),
+          [num_blocks] "r"(num_blocks), [rhs_packed] "r"(rhs_packed)
+        : "cc", "memory", "v0", "v1", "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23", "v24", "v25", "v26",
+          "v27", "v28", "v29", "v30", "v31", "x20", "x21", "x22", "x23", "x24", "x25", "x26");
+}
+
+#endif  // Architectural feature check
diff --git a/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h
new file mode 100644
index 0000000..c49f89b
--- /dev/null
+++ b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.h
@@ -0,0 +1,143 @@
+
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#pragma once
+
+#include <stddef.h>
+#include "kai/kai_common.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/// Micro-kernel dependencies
+///
+/// -# kai_lhs_quant_pack_qsi8d32p_f32 to dynamically quantize and pack the LHS matrix
+/// -# kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0 to pack the RHS matrix
+
+/// --------------------------------------------------
+
+/// Gets the m step value.
+/// The micro-kernel can process any M values. However, the starting M index to
+/// be processed must be a multiple of m step.
+///
+/// @return the m step value
+size_t kai_get_m_step_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the n step value.
+/// The micro-kernel can process any N values. However, the starting N index to
+/// be processed must be a multiple of n step.
+///
+/// @return the n step
+size_t kai_get_n_step_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the mr value, which must be used to pack the LHS matrix
+///
+/// @return the mr value
+size_t kai_get_mr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the nr value, which must be used to pack the RHS matrix.
+///
+/// @return the nr value
+size_t kai_get_nr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the kr value, which must be used to pack the LHS and RHS matrices
+///
+/// @return the kr value
+size_t kai_get_kr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the sr value, which must be used to pack the LHS and RHS matrices
+///
+/// @return the sr value
+size_t kai_get_sr_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(void);
+
+/// Gets the offset in bytes for the packed LHS matrix,
+/// which contains the packed Signed 8-bit quantized symmetric per-block (qsi8d32) values.
+///
+/// This function should be called before passing the pointer to the packed LHS matrix to the micro-kernel.
+///
+/// @param[in] m_idx Row index in the LHS matrix (not packed). It must be a multiple of 1
+/// @param[in] k     Total number of columns in the LHS matrix (not packed).
+///
+/// @return the offset in bytes to the packed LHS matrix
+size_t kai_get_lhs_packed_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m_idx,  //
+    size_t k,      //
+    size_t bl);    //
+
+/// Gets the offset in bytes for the packed RHS matrix,
+/// which contains the packed Signed 4-bit quantized symmetric per-block (qsi4c32) values.
+///
+/// @param[in] n_idx Row index in the RHS matrix (not packed). It must be a multiple of 4.
+/// @param[in] k     The common dimension between the LHS and RHS matrix (K).
+/// @param[in] bl    Block length. It must be 32.
+///
+/// @return the offset in bytes to the packed RHS matrix
+size_t kai_get_rhs_packed_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t n_idx,  //
+    size_t k,      //
+    size_t bl);    //
+
+/// Gets the offset in bytes for the DST matrix
+///
+/// @param[in] m_idx      Row index in the DST matrix. It must be a multiple of 1.
+/// @param[in] n_idx      Column index in the DST matrix. It must be multiple of 4.
+/// @param[in] dst_stride The number of bytes in in each row of the DST matrix
+///
+/// @return the DST offset in bytes
+size_t kai_get_dst_offset_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m_idx,        //
+    size_t n_idx,        //
+    size_t dst_stride);  //
+
+/// Gets the size in bytes for the destination (DST) matrix.
+///
+/// @param[in] m Number of rows in the destination (DST) matrix.
+/// @param[in] n Number of columns in the destination (DST) matrix.
+///
+/// @return the destination (DST) matrix size in bytes
+size_t kai_get_dst_size_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m,   //
+    size_t n);  //
+
+/// Runs the matrix multiplication (matmul) micro-kernel followed by a clamp (min-max) operation.
+///
+/// LHS matrix: Signed 8-bit quantized symmetric per-block (qsi8d32) and packed
+/// RHS matrix: Signed 4-bit quantized symmetric per-block (qsi4c32) and packed.
+/// Output tile: (rows x cols) = 1 x 4
+/// Accumulation performed in a single for loop: 32
+/// Extension used: dotprod
+///
+/// @param[in]  m              The number of output rows written.
+/// @param[in]  n              The number of output columns written.
+/// @param[in]  k              The number of channels. The common dimension between the LHS and RHS matrix.
+/// @param[in]  bl             Block length. It must be 32.
+/// @param[in]  lhs_packed     The LHS packed matrix.
+///                            When the activation are dynamically quantized, you can obtain this matrix
+///                            by calling the @ref kai_lhs_quant_pack_qsi8d32p_f32 micro-kernel which performs
+///                            both the dynamic quantization to 8-bit and activation packing in a single step.
+/// @param[in]  rhs_packed     The RHS packed matrix, which is obtained by calling @ref
+/// kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0
+/// @param[out] dst            The DST matrix.
+/// @param[in]  dst_stride_row Stride in bytes between two rows of the DST matrix.
+/// @param[in]  dst_stride_col Stride in bytes between two columns of the DST matrix. It must be sizeof(float16_t).
+/// @param[in]  scalar_min     Min value used to clamp the final result.
+/// @param[in]  scalar_max     Max value used to clamp the final result.
+void kai_run_matmul_clamp_f16_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod(
+    size_t m,                //
+    size_t n,                //
+    size_t k,                //
+    size_t bl,               //
+    const void* lhs_packed,  //
+    const void* rhs_packed,  //
+    float16_t* dst,              //
+    size_t dst_stride_row,   //
+    size_t dst_stride_col,   //
+    float scalar_min,        //
+    float scalar_max);       //
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c
new file mode 100644
index 0000000..95c1f67
--- /dev/null
+++ b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c
@@ -0,0 +1,701 @@
+
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#if !defined(__ARM_FEATURE_MATMUL_INT8)
+#error "i8mm extension required to compile this micro-kernel"
+#else
+#include "kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h"
+
+#include <arm_neon.h>
+#include <stddef.h>
+#include <stdint.h>
+
+#include "kai/kai_common.h"
+
+static const size_t kai_m_step = 16;
+static const size_t kai_n_step = 4;
+static const size_t kai_mr = 4;
+static const size_t kai_nr = 4;
+static const size_t kai_kr = 16;
+static const size_t kai_sr = 2;
+static const size_t kai_bl = 32;
+static const size_t kai_num_bytes_multiplier = sizeof(uint16_t);
+
+inline static size_t kai_num_bytes_per_block_lhs(void) {
+    return kai_bl * sizeof(int8_t) + kai_num_bytes_multiplier;
+}
+
+inline static size_t kai_num_bytes_per_block_rhs(void) {
+    return (kai_bl / 2) * sizeof(int8_t) + kai_num_bytes_multiplier;
+}
+
+inline static size_t kai_num_blocks_per_row(size_t k) {
+    KAI_ASSUME((k % kai_bl) == 0);
+    return k / kai_bl;
+}
+
+inline static size_t kai_lhs_packed_stride(size_t k) {
+    return kai_mr * kai_num_blocks_per_row(k) * kai_num_bytes_per_block_lhs();
+}
+
+inline static size_t kai_rhs_packed_stride(size_t k) {
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % kai_bl) == 0);
+
+    const size_t num_blocks_per_row = kai_num_blocks_per_row(k);
+    const size_t num_bytes_per_block = kai_num_bytes_per_block_rhs();
+
+    return kai_nr * (num_bytes_per_block * num_blocks_per_row);
+}
+
+size_t kai_get_m_step_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_m_step;
+}
+
+size_t kai_get_n_step_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_n_step;
+}
+
+size_t kai_get_mr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_mr;
+}
+
+size_t kai_get_nr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_nr;
+}
+
+size_t kai_get_kr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_kr;
+}
+
+size_t kai_get_sr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void) {
+    return kai_sr;
+}
+
+size_t kai_get_lhs_packed_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m_idx, size_t k, size_t bl) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+    KAI_ASSUME((m_idx % kai_m_step) == 0);
+
+    return (m_idx / kai_m_step) * kai_lhs_packed_stride(k);
+}
+
+size_t kai_get_rhs_packed_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t n_idx, size_t k, size_t bl) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kai_kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+    KAI_ASSUME((n_idx % kai_n_step) == 0);
+
+    return (n_idx / kai_n_step) * kai_rhs_packed_stride(k);
+}
+
+size_t kai_get_dst_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m_idx, size_t n_idx, size_t dst_stride) {
+    KAI_ASSUME((m_idx % kai_m_step) == 0);
+    KAI_ASSUME((n_idx % kai_n_step) == 0);
+
+    return (n_idx * sizeof(float16_t)) + m_idx * dst_stride;
+}
+
+size_t kai_get_dst_size_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(size_t m, size_t n) {
+    return m * n * sizeof(float16_t);
+}
+
+void kai_run_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m, size_t n, size_t k, size_t bl, const void* lhs_packed, const void* rhs_packed,
+    float16_t* dst,  // NOLINT(readability-non-const-parameter)
+    size_t dst_stride_row, size_t dst_stride_col, float scalar_min, float scalar_max) {
+    KAI_ASSUME(bl == kai_bl);
+    KAI_ASSUME(k % kai_bl == 0);
+    KAI_ASSUME(dst_stride_col == sizeof(float16_t));
+
+    if (m == 0) {
+        return;
+    }
+
+    const size_t num_blocks = k / kai_bl;
+    float clamp_vals[2] = {scalar_min, scalar_max};
+
+    __asm__ __volatile__(
+        "mov x13, %x[m]\n"
+        "mov x12, #0x88\n"
+        "cmp x13, #0x10\n"
+        "mul x12, %x[num_blocks], x12\n"
+        "blt 14f\n"
+        "1:"  // Row loop
+        "mov x11, %x[rhs_packed]\n"
+        "mov x10, %x[n]\n"
+        "add x9, %x[dst], %x[dst_stride_row], LSL #4\n"
+        "2:"  // Column loop
+        "mov x27, %x[lhs_packed]\n"
+        "movi v31.16b, #0x0\n"
+        "movi v30.16b, #0x0\n"
+        "mov x23, %x[num_blocks]\n"
+        "movi v29.16b, #0x0\n"
+        "movi v28.16b, #0x0\n"
+        "movi v27.16b, #0x0\n"
+        "movi v26.16b, #0x0\n"
+        "add x22, x27, x12\n"
+        "add x21, x22, x12\n"
+        "movi v25.16b, #0x0\n"
+        "movi v24.16b, #0x0\n"
+        "add x20, x21, x12\n"
+        "movi v23.16b, #0x0\n"
+        "movi v22.16b, #0x0\n"
+        "movi v21.16b, #0x0\n"
+        "movi v20.16b, #0x0\n"
+        "movi v19.16b, #0x0\n"
+        "movi v18.16b, #0x0\n"
+        "movi v17.16b, #0x0\n"
+        "movi v16.16b, #0x0\n"
+        "3:"  // Block loop
+        "ldr d0, [x11, #0x0]\n"
+        "ldr d3, [x27, #0x0]\n"
+        "add x11, x11, #0x8\n"
+        "add x27, x27, #0x8\n"
+        "ldr q12, [x11, #0x0]\n"
+        "ldr q4, [x11, #0x10]\n"
+        "movi v5.4s, #0x0\n"
+        "movi v14.4s, #0x0\n"
+        "ldr q9, [x27, #0x0]\n"
+        "ldr q10, [x27, #0x10]\n"
+        "movi v7.4s, #0x0\n"
+        "movi v8.4s, #0x0\n"
+        "ldr q2, [x11, #0x20]\n"
+        "ldr q11, [x11, #0x30]\n"
+        "movi v1.16b, #0xf0\n"
+        "fcvtl v6.4s, v0.4h\n"
+        "ldr q15, [x27, #0x20]\n"
+        "shl v13.16b, v12.16b, #0x4\n"
+        "shl v0.16b, v4.16b, #0x4\n"
+        "add x11, x11, #0x40\n"
+        "and v12.16b, v12.16b, v1.16b\n"
+        "and v4.16b, v4.16b, v1.16b\n"
+        "fcvtl v3.4s, v3.4h\n"
+        ".inst 0x4e8da525  // smmla v5.4s, v9.16b, v13.16b\n"
+        ".inst 0x4e80a52e  // smmla v14.4s, v9.16b, v0.16b\n"
+        ".inst 0x4e8da547  // smmla v7.4s, v10.16b, v13.16b\n"
+        ".inst 0x4e80a548  // smmla v8.4s, v10.16b, v0.16b\n"
+        "shl v10.16b, v2.16b, #0x4\n"
+        "shl v9.16b, v11.16b, #0x4\n"
+        "and v2.16b, v2.16b, v1.16b\n"
+        "and v11.16b, v11.16b, v1.16b\n"
+        "ldr q1, [x27, #0x30]\n"
+        ".inst 0x4e8aa5e5  // smmla v5.4s, v15.16b, v10.16b\n"
+        ".inst 0x4e89a5ee  // smmla v14.4s, v15.16b, v9.16b\n"
+        "ldr q15, [x27, #0x40]\n"
+        ".inst 0x4e8aa427  // smmla v7.4s, v1.16b, v10.16b\n"
+        ".inst 0x4e89a428  // smmla v8.4s, v1.16b, v9.16b\n"
+        "ldr q1, [x27, #0x50]\n"
+        ".inst 0x4e8ca5e5  // smmla v5.4s, v15.16b, v12.16b\n"
+        ".inst 0x4e84a5ee  // smmla v14.4s, v15.16b, v4.16b\n"
+        "ldr q15, [x27, #0x60]\n"
+        ".inst 0x4e8ca427  // smmla v7.4s, v1.16b, v12.16b\n"
+        ".inst 0x4e84a428  // smmla v8.4s, v1.16b, v4.16b\n"
+        "ldr q1, [x27, #0x70]\n"
+        "add x27, x27, #0x80\n"
+        ".inst 0x4e82a5e5  // smmla v5.4s, v15.16b, v2.16b\n"
+        ".inst 0x4e8ba5ee  // smmla v14.4s, v15.16b, v11.16b\n"
+        "fmul v15.4s, v6.4s, v3.s[0]\n"
+        ".inst 0x4e82a427  // smmla v7.4s, v1.16b, v2.16b\n"
+        ".inst 0x4e8ba428  // smmla v8.4s, v1.16b, v11.16b\n"
+        "uzp1 v1.2d, v5.2d, v14.2d\n"
+        "uzp2 v5.2d, v5.2d, v14.2d\n"
+        "fmul v14.4s, v6.4s, v3.s[1]\n"
+        "scvtf v1.4s, v1.4s, #0x4\n"
+        "scvtf v5.4s, v5.4s, #0x4\n"
+        "fmla v31.4s, v1.4s, v15.4s\n"
+        "fmul v15.4s, v6.4s, v3.s[2]\n"
+        "fmul v3.4s, v6.4s, v3.s[3]\n"
+        "uzp1 v1.2d, v7.2d, v8.2d\n"
+        "uzp2 v8.2d, v7.2d, v8.2d\n"
+        "fmla v30.4s, v5.4s, v14.4s\n"
+        "scvtf v1.4s, v1.4s, #0x4\n"
+        "scvtf v8.4s, v8.4s, #0x4\n"
+        "fmla v29.4s, v1.4s, v15.4s\n"
+        "fmla v28.4s, v8.4s, v3.4s\n"
+        "ldr d5, [x22, #0x0]\n"
+        "add x22, x22, #0x8\n"
+        "movi v3.4s, #0x0\n"
+        "movi v1.4s, #0x0\n"
+        "ldr q15, [x22, #0x0]\n"
+        "ldr q7, [x22, #0x10]\n"
+        "movi v14.4s, #0x0\n"
+        "movi v8.4s, #0x0\n"
+        "fcvtl v5.4s, v5.4h\n"
+        ".inst 0x4e8da5e3  // smmla v3.4s, v15.16b, v13.16b\n"
+        ".inst 0x4e80a5e1  // smmla v1.4s, v15.16b, v0.16b\n"
+        "ldr q15, [x22, #0x20]\n"
+        ".inst 0x4e8da4ee  // smmla v14.4s, v7.16b, v13.16b\n"
+        ".inst 0x4e80a4e8  // smmla v8.4s, v7.16b, v0.16b\n"
+        "ldr q7, [x22, #0x30]\n"
+        ".inst 0x4e8aa5e3  // smmla v3.4s, v15.16b, v10.16b\n"
+        ".inst 0x4e89a5e1  // smmla v1.4s, v15.16b, v9.16b\n"
+        "ldr q15, [x22, #0x40]\n"
+        ".inst 0x4e8aa4ee  // smmla v14.4s, v7.16b, v10.16b\n"
+        ".inst 0x4e89a4e8  // smmla v8.4s, v7.16b, v9.16b\n"
+        "ldr q7, [x22, #0x50]\n"
+        ".inst 0x4e8ca5e3  // smmla v3.4s, v15.16b, v12.16b\n"
+        ".inst 0x4e84a5e1  // smmla v1.4s, v15.16b, v4.16b\n"
+        "ldr q15, [x22, #0x60]\n"
+        ".inst 0x4e8ca4ee  // smmla v14.4s, v7.16b, v12.16b\n"
+        ".inst 0x4e84a4e8  // smmla v8.4s, v7.16b, v4.16b\n"
+        "ldr q7, [x22, #0x70]\n"
+        "add x22, x22, #0x80\n"
+        ".inst 0x4e82a5e3  // smmla v3.4s, v15.16b, v2.16b\n"
+        ".inst 0x4e8ba5e1  // smmla v1.4s, v15.16b, v11.16b\n"
+        "fmul v15.4s, v6.4s, v5.s[0]\n"
+        ".inst 0x4e82a4ee  // smmla v14.4s, v7.16b, v2.16b\n"
+        ".inst 0x4e8ba4e8  // smmla v8.4s, v7.16b, v11.16b\n"
+        "uzp1 v7.2d, v3.2d, v1.2d\n"
+        "uzp2 v1.2d, v3.2d, v1.2d\n"
+        "fmul v3.4s, v6.4s, v5.s[1]\n"
+        "scvtf v7.4s, v7.4s, #0x4\n"
+        "scvtf v1.4s, v1.4s, #0x4\n"
+        "fmla v27.4s, v7.4s, v15.4s\n"
+        "fmul v15.4s, v6.4s, v5.s[2]\n"
+        "fmul v7.4s, v6.4s, v5.s[3]\n"
+        "uzp1 v5.2d, v14.2d, v8.2d\n"
+        "uzp2 v14.2d, v14.2d, v8.2d\n"
+        "fmla v26.4s, v1.4s, v3.4s\n"
+        "scvtf v5.4s, v5.4s, #0x4\n"
+        "scvtf v14.4s, v14.4s, #0x4\n"
+        "fmla v25.4s, v5.4s, v15.4s\n"
+        "fmla v24.4s, v14.4s, v7.4s\n"
+        "ldr d1, [x21, #0x0]\n"
+        "add x21, x21, #0x8\n"
+        "movi v8.4s, #0x0\n"
+        "movi v5.4s, #0x0\n"
+        "ldr q3, [x21, #0x0]\n"
+        "ldr q7, [x21, #0x10]\n"
+        "movi v14.4s, #0x0\n"
+        "movi v15.4s, #0x0\n"
+        "fcvtl v1.4s, v1.4h\n"
+        ".inst 0x4e8da468  // smmla v8.4s, v3.16b, v13.16b\n"
+        ".inst 0x4e80a465  // smmla v5.4s, v3.16b, v0.16b\n"
+        "ldr q3, [x21, #0x20]\n"
+        ".inst 0x4e8da4ee  // smmla v14.4s, v7.16b, v13.16b\n"
+        ".inst 0x4e80a4ef  // smmla v15.4s, v7.16b, v0.16b\n"
+        "ldr q7, [x21, #0x30]\n"
+        ".inst 0x4e8aa468  // smmla v8.4s, v3.16b, v10.16b\n"
+        ".inst 0x4e89a465  // smmla v5.4s, v3.16b, v9.16b\n"
+        "ldr q3, [x21, #0x40]\n"
+        ".inst 0x4e8aa4ee  // smmla v14.4s, v7.16b, v10.16b\n"
+        ".inst 0x4e89a4ef  // smmla v15.4s, v7.16b, v9.16b\n"
+        "ldr q7, [x21, #0x50]\n"
+        ".inst 0x4e8ca468  // smmla v8.4s, v3.16b, v12.16b\n"
+        ".inst 0x4e84a465  // smmla v5.4s, v3.16b, v4.16b\n"
+        "ldr q3, [x21, #0x60]\n"
+        ".inst 0x4e8ca4ee  // smmla v14.4s, v7.16b, v12.16b\n"
+        ".inst 0x4e84a4ef  // smmla v15.4s, v7.16b, v4.16b\n"
+        "ldr q7, [x21, #0x70]\n"
+        "add x21, x21, #0x80\n"
+        ".inst 0x4e82a468  // smmla v8.4s, v3.16b, v2.16b\n"
+        ".inst 0x4e8ba465  // smmla v5.4s, v3.16b, v11.16b\n"
+        "fmul v3.4s, v6.4s, v1.s[0]\n"
+        ".inst 0x4e82a4ee  // smmla v14.4s, v7.16b, v2.16b\n"
+        ".inst 0x4e8ba4ef  // smmla v15.4s, v7.16b, v11.16b\n"
+        "uzp1 v7.2d, v8.2d, v5.2d\n"
+        "uzp2 v8.2d, v8.2d, v5.2d\n"
+        "fmul v5.4s, v6.4s, v1.s[1]\n"
+        "scvtf v7.4s, v7.4s, #0x4\n"
+        "scvtf v8.4s, v8.4s, #0x4\n"
+        "fmla v23.4s, v7.4s, v3.4s\n"
+        "fmul v3.4s, v6.4s, v1.s[2]\n"
+        "fmul v1.4s, v6.4s, v1.s[3]\n"
+        "uzp1 v7.2d, v14.2d, v15.2d\n"
+        "uzp2 v14.2d, v14.2d, v15.2d\n"
+        "fmla v22.4s, v8.4s, v5.4s\n"
+        "scvtf v7.4s, v7.4s, #0x4\n"
+        "scvtf v14.4s, v14.4s, #0x4\n"
+        "fmla v21.4s, v7.4s, v3.4s\n"
+        "fmla v20.4s, v14.4s, v1.4s\n"
+        "ldr d3, [x20, #0x0]\n"
+        "add x20, x20, #0x8\n"
+        "movi v15.4s, #0x0\n"
+        "movi v8.4s, #0x0\n"
+        "ldr q5, [x20, #0x0]\n"
+        "ldr q14, [x20, #0x10]\n"
+        "movi v1.4s, #0x0\n"
+        "movi v7.4s, #0x0\n"
+        "fcvtl v3.4s, v3.4h\n"
+        ".inst 0x4e8da4af  // smmla v15.4s, v5.16b, v13.16b\n"
+        ".inst 0x4e80a4a8  // smmla v8.4s, v5.16b, v0.16b\n"
+        "ldr q5, [x20, #0x20]\n"
+        ".inst 0x4e8da5c1  // smmla v1.4s, v14.16b, v13.16b\n"
+        "ldr q13, [x20, #0x30]\n"
+        ".inst 0x4e80a5c7  // smmla v7.4s, v14.16b, v0.16b\n"
+        "ldr q14, [x20, #0x40]\n"
+        "ldr q0, [x20, #0x50]\n"
+        ".inst 0x4e8aa4af  // smmla v15.4s, v5.16b, v10.16b\n"
+        ".inst 0x4e89a4a8  // smmla v8.4s, v5.16b, v9.16b\n"
+        "ldr q5, [x20, #0x60]\n"
+        ".inst 0x4e8aa5a1  // smmla v1.4s, v13.16b, v10.16b\n"
+        "ldr q10, [x20, #0x70]\n"
+        "add x20, x20, #0x80\n"
+        ".inst 0x4e89a5a7  // smmla v7.4s, v13.16b, v9.16b\n"
+        "fmul v13.4s, v6.4s, v3.s[0]\n"
+        "fmul v9.4s, v6.4s, v3.s[1]\n"
+        ".inst 0x4e8ca5cf  // smmla v15.4s, v14.16b, v12.16b\n"
+        ".inst 0x4e84a5c8  // smmla v8.4s, v14.16b, v4.16b\n"
+        "fmul v14.4s, v6.4s, v3.s[2]\n"
+        "fmul v6.4s, v6.4s, v3.s[3]\n"
+        ".inst 0x4e8ca401  // smmla v1.4s, v0.16b, v12.16b\n"
+        ".inst 0x4e84a407  // smmla v7.4s, v0.16b, v4.16b\n"
+        ".inst 0x4e82a4af  // smmla v15.4s, v5.16b, v2.16b\n"
+        ".inst 0x4e8ba4a8  // smmla v8.4s, v5.16b, v11.16b\n"
+        ".inst 0x4e82a541  // smmla v1.4s, v10.16b, v2.16b\n"
+        ".inst 0x4e8ba547  // smmla v7.4s, v10.16b, v11.16b\n"
+        "uzp1 v4.2d, v15.2d, v8.2d\n"
+        "uzp2 v2.2d, v15.2d, v8.2d\n"
+        "scvtf v4.4s, v4.4s, #0x4\n"
+        "uzp1 v8.2d, v1.2d, v7.2d\n"
+        "uzp2 v0.2d, v1.2d, v7.2d\n"
+        "scvtf v2.4s, v2.4s, #0x4\n"
+        "fmla v19.4s, v4.4s, v13.4s\n"
+        "scvtf v8.4s, v8.4s, #0x4\n"
+        "scvtf v0.4s, v0.4s, #0x4\n"
+        "fmla v18.4s, v2.4s, v9.4s\n"
+        "fmla v17.4s, v8.4s, v14.4s\n"
+        "fmla v16.4s, v0.4s, v6.4s\n"
+        "subs x23, x23, #0x1\n"
+        "bgt 3b\n"
+        "ld1r { v1.4s }, [%x[clamp_vals]]\n"
+        "add x20, %x[clamp_vals], #0x4\n"
+        "cmp x10, #0x4\n"
+        "ld1r { v0.4s }, [x20]\n"
+        "fmax v31.4s, v31.4s, v1.4s\n"
+        "fmax v30.4s, v30.4s, v1.4s\n"
+        "fmax v29.4s, v29.4s, v1.4s\n"
+        "fmax v28.4s, v28.4s, v1.4s\n"
+        "fmax v27.4s, v27.4s, v1.4s\n"
+        "fmax v26.4s, v26.4s, v1.4s\n"
+        "fmax v25.4s, v25.4s, v1.4s\n"
+        "fmax v24.4s, v24.4s, v1.4s\n"
+        "fmax v23.4s, v23.4s, v1.4s\n"
+        "fmax v22.4s, v22.4s, v1.4s\n"
+        "fmax v21.4s, v21.4s, v1.4s\n"
+        "fmax v20.4s, v20.4s, v1.4s\n"
+        "fmax v19.4s, v19.4s, v1.4s\n"
+        "fmax v18.4s, v18.4s, v1.4s\n"
+        "fmax v17.4s, v17.4s, v1.4s\n"
+        "fmax v16.4s, v16.4s, v1.4s\n"
+        "fmin v31.4s, v31.4s, v0.4s\n"
+        "fmin v30.4s, v30.4s, v0.4s\n"
+        "fmin v29.4s, v29.4s, v0.4s\n"
+        "fmin v28.4s, v28.4s, v0.4s\n"
+        "fmin v27.4s, v27.4s, v0.4s\n"
+        "fmin v26.4s, v26.4s, v0.4s\n"
+        "fmin v25.4s, v25.4s, v0.4s\n"
+        "fmin v24.4s, v24.4s, v0.4s\n"
+        "fmin v23.4s, v23.4s, v0.4s\n"
+        "fmin v22.4s, v22.4s, v0.4s\n"
+        "fmin v21.4s, v21.4s, v0.4s\n"
+        "fmin v20.4s, v20.4s, v0.4s\n"
+        "fmin v19.4s, v19.4s, v0.4s\n"
+        "fmin v18.4s, v18.4s, v0.4s\n"
+        "fmin v17.4s, v17.4s, v0.4s\n"
+        "fmin v16.4s, v16.4s, v0.4s\n"
+        "fcvtn v31.4h, v31.4s\n"
+        "fcvtn v30.4h, v30.4s\n"
+        "fcvtn v29.4h, v29.4s\n"
+        "fcvtn v28.4h, v28.4s\n"
+        "fcvtn v27.4h, v27.4s\n"
+        "fcvtn v26.4h, v26.4s\n"
+        "fcvtn v25.4h, v25.4s\n"
+        "fcvtn v24.4h, v24.4s\n"
+        "fcvtn v23.4h, v23.4s\n"
+        "fcvtn v22.4h, v22.4s\n"
+        "fcvtn v21.4h, v21.4s\n"
+        "fcvtn v20.4h, v20.4s\n"
+        "fcvtn v19.4h, v19.4s\n"
+        "fcvtn v18.4h, v18.4s\n"
+        "fcvtn v17.4h, v17.4s\n"
+        "fcvtn v16.4h, v16.4s\n"
+        "blt 8f\n"
+        "mov x20, %x[dst]\n"
+        "str d31, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d30, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d29, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d28, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d27, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d26, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d25, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d24, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d23, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d22, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d21, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d20, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d19, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d18, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d17, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "str d16, [x20, #0x0]\n"
+        "b 13f\n"
+        "8:"  // Partial output
+        "mov x28, %x[dst]\n"
+        "add x26, x28, %x[dst_stride_row], LSL #2\n"
+        "add x25, x26, %x[dst_stride_row], LSL #1\n"
+        "add x24, x26, %x[dst_stride_row]\n"
+        "add x23, x25, %x[dst_stride_row]\n"
+        "add x22, x28, %x[dst_stride_row], LSL #1\n"
+        "add x21, x28, %x[dst_stride_row]\n"
+        "add x20, x22, %x[dst_stride_row]\n"
+        "add x27, x23, %x[dst_stride_row]\n"
+        "tbz x10, #1, 9f\n"
+        "st1 { v24.s }[0], [x23], #0x4\n"
+        "st1 { v25.s }[0], [x25], #0x4\n"
+        "st1 { v26.s }[0], [x24], #0x4\n"
+        "st1 { v27.s }[0], [x26], #0x4\n"
+        "st1 { v28.s }[0], [x20], #0x4\n"
+        "st1 { v29.s }[0], [x22], #0x4\n"
+        "st1 { v30.s }[0], [x21], #0x4\n"
+        "st1 { v31.s }[0], [x28], #0x4\n"
+        "tbz x10, #0, 10f\n"
+        "st1 { v24.h }[2], [x23]\n"
+        "st1 { v25.h }[2], [x25]\n"
+        "st1 { v26.h }[2], [x24]\n"
+        "st1 { v27.h }[2], [x26]\n"
+        "st1 { v28.h }[2], [x20]\n"
+        "st1 { v29.h }[2], [x22]\n"
+        "st1 { v30.h }[2], [x21]\n"
+        "st1 { v31.h }[2], [x28]\n"
+        "b 10f\n"
+        "9:"  // Output block 0: partial_1_0
+        "st1 { v24.h }[0], [x23]\n"
+        "st1 { v25.h }[0], [x25]\n"
+        "st1 { v26.h }[0], [x24]\n"
+        "st1 { v27.h }[0], [x26]\n"
+        "st1 { v28.h }[0], [x20]\n"
+        "st1 { v29.h }[0], [x22]\n"
+        "st1 { v30.h }[0], [x21]\n"
+        "st1 { v31.h }[0], [x28]\n"
+        "10:"  // Output block 0: Done
+        "add x26, x27, %x[dst_stride_row], LSL #2\n"
+        "add x25, x27, %x[dst_stride_row], LSL #1\n"
+        "add x24, x26, %x[dst_stride_row], LSL #1\n"
+        "add x23, x27, %x[dst_stride_row]\n"
+        "add x22, x25, %x[dst_stride_row]\n"
+        "add x21, x26, %x[dst_stride_row]\n"
+        "add x20, x24, %x[dst_stride_row]\n"
+        "tbz x10, #1, 11f\n"
+        "st1 { v16.s }[0], [x20], #0x4\n"
+        "st1 { v17.s }[0], [x24], #0x4\n"
+        "st1 { v18.s }[0], [x21], #0x4\n"
+        "st1 { v19.s }[0], [x26], #0x4\n"
+        "st1 { v20.s }[0], [x22], #0x4\n"
+        "st1 { v21.s }[0], [x25], #0x4\n"
+        "st1 { v22.s }[0], [x23], #0x4\n"
+        "st1 { v23.s }[0], [x27], #0x4\n"
+        "tbz x10, #0, 12f\n"
+        "st1 { v16.h }[2], [x20]\n"
+        "st1 { v17.h }[2], [x24]\n"
+        "st1 { v18.h }[2], [x21]\n"
+        "st1 { v19.h }[2], [x26]\n"
+        "st1 { v20.h }[2], [x22]\n"
+        "st1 { v21.h }[2], [x25]\n"
+        "st1 { v22.h }[2], [x23]\n"
+        "st1 { v23.h }[2], [x27]\n"
+        "b 12f\n"
+        "11:"  // Output block 1: partial_1_0
+        "st1 { v16.h }[0], [x20]\n"
+        "st1 { v17.h }[0], [x24]\n"
+        "st1 { v18.h }[0], [x21]\n"
+        "st1 { v19.h }[0], [x26]\n"
+        "st1 { v20.h }[0], [x22]\n"
+        "st1 { v21.h }[0], [x25]\n"
+        "st1 { v22.h }[0], [x23]\n"
+        "st1 { v23.h }[0], [x27]\n"
+        "12:"  // Output block 1: Done
+        "13:"  // Output stage exit
+        "subs x10, x10, #0x4\n"
+        "add %x[dst], %x[dst], #0x08\n"
+        "bgt 2b\n"
+        "mov x20, #0x4\n"
+        "sub x13, x13, #0x10\n"
+        "cmp x13, #0x10\n"
+        "mov %x[dst], x9\n"
+        "madd %x[lhs_packed], x20, x12, %x[lhs_packed]\n"
+        "bge 1b\n"
+        "14:"  // Row loop skip
+        "cbz x13, 23f\n"
+        "15:"  // Row tail: Row loop
+        "mov x26, %x[rhs_packed]\n"
+        "mov x25, %x[n]\n"
+        "add x24, %x[dst], %x[dst_stride_row], LSL #2\n"
+        "16:"  // Row tail: Column loop
+        "movi v31.16b, #0x0\n"
+        "movi v30.16b, #0x0\n"
+        "mov x27, %x[lhs_packed]\n"
+        "mov x20, %x[num_blocks]\n"
+        "movi v29.16b, #0x0\n"
+        "movi v28.16b, #0x0\n"
+        "17:"  // Row tail: Block loop
+        "ldr d16, [x26, #0x0]\n"
+        "ldr d10, [x27, #0x0]\n"
+        "add x26, x26, #0x8\n"
+        "add x27, x27, #0x8\n"
+        "ldr q9, [x26, #0x0]\n"
+        "ldr q8, [x26, #0x10]\n"
+        "movi v7.4s, #0x0\n"
+        "movi v6.4s, #0x0\n"
+        "ldr q5, [x27, #0x0]\n"
+        "ldr q4, [x27, #0x10]\n"
+        "movi v3.4s, #0x0\n"
+        "movi v2.4s, #0x0\n"
+        "ldr q1, [x26, #0x20]\n"
+        "ldr q0, [x26, #0x30]\n"
+        "movi v27.16b, #0xf0\n"
+        "fcvtl v26.4s, v16.4h\n"
+        "ldr q23, [x27, #0x20]\n"
+        "ldr q22, [x27, #0x30]\n"
+        "shl v21.16b, v9.16b, #0x4\n"
+        "shl v20.16b, v8.16b, #0x4\n"
+        "ldr q25, [x27, #0x40]\n"
+        "ldr q24, [x27, #0x50]\n"
+        "and v9.16b, v9.16b, v27.16b\n"
+        "and v8.16b, v8.16b, v27.16b\n"
+        "ldr q19, [x27, #0x60]\n"
+        "ldr q18, [x27, #0x70]\n"
+        "shl v17.16b, v1.16b, #0x4\n"
+        "shl v16.16b, v0.16b, #0x4\n"
+        ".inst 0x4e95a4a7  // smmla v7.4s, v5.16b, v21.16b\n"
+        ".inst 0x4e94a4a6  // smmla v6.4s, v5.16b, v20.16b\n"
+        "and v1.16b, v1.16b, v27.16b\n"
+        "add x26, x26, #0x40\n"
+        ".inst 0x4e95a483  // smmla v3.4s, v4.16b, v21.16b\n"
+        ".inst 0x4e94a482  // smmla v2.4s, v4.16b, v20.16b\n"
+        "and v0.16b, v0.16b, v27.16b\n"
+        "add x27, x27, #0x80\n"
+        "fcvtl v10.4s, v10.4h\n"
+        ".inst 0x4e91a6e7  // smmla v7.4s, v23.16b, v17.16b\n"
+        ".inst 0x4e90a6e6  // smmla v6.4s, v23.16b, v16.16b\n"
+        ".inst 0x4e91a6c3  // smmla v3.4s, v22.16b, v17.16b\n"
+        ".inst 0x4e90a6c2  // smmla v2.4s, v22.16b, v16.16b\n"
+        "fmul v23.4s, v26.4s, v10.s[0]\n"
+        "fmul v22.4s, v26.4s, v10.s[1]\n"
+        "fmul v21.4s, v26.4s, v10.s[2]\n"
+        "fmul v20.4s, v26.4s, v10.s[3]\n"
+        ".inst 0x4e89a727  // smmla v7.4s, v25.16b, v9.16b\n"
+        ".inst 0x4e88a726  // smmla v6.4s, v25.16b, v8.16b\n"
+        ".inst 0x4e89a703  // smmla v3.4s, v24.16b, v9.16b\n"
+        ".inst 0x4e88a702  // smmla v2.4s, v24.16b, v8.16b\n"
+        ".inst 0x4e81a667  // smmla v7.4s, v19.16b, v1.16b\n"
+        ".inst 0x4e80a666  // smmla v6.4s, v19.16b, v0.16b\n"
+        ".inst 0x4e81a643  // smmla v3.4s, v18.16b, v1.16b\n"
+        ".inst 0x4e80a642  // smmla v2.4s, v18.16b, v0.16b\n"
+        "uzp1 v19.2d, v7.2d, v6.2d\n"
+        "uzp2 v18.2d, v7.2d, v6.2d\n"
+        "scvtf v19.4s, v19.4s, #0x4\n"
+        "uzp1 v17.2d, v3.2d, v2.2d\n"
+        "uzp2 v16.2d, v3.2d, v2.2d\n"
+        "scvtf v18.4s, v18.4s, #0x4\n"
+        "fmla v31.4s, v19.4s, v23.4s\n"
+        "scvtf v17.4s, v17.4s, #0x4\n"
+        "scvtf v16.4s, v16.4s, #0x4\n"
+        "fmla v30.4s, v18.4s, v22.4s\n"
+        "fmla v29.4s, v17.4s, v21.4s\n"
+        "fmla v28.4s, v16.4s, v20.4s\n"
+        "subs x20, x20, #0x1\n"
+        "bgt 17b\n"
+        "ld1r { v17.4s }, [%x[clamp_vals]]\n"
+        "add x20, %x[clamp_vals], #0x4\n"
+        "cmp x25, #0x4\n"
+        "ld1r { v16.4s }, [x20]\n"
+        "fmax v31.4s, v31.4s, v17.4s\n"
+        "fmax v30.4s, v30.4s, v17.4s\n"
+        "fmax v29.4s, v29.4s, v17.4s\n"
+        "fmax v28.4s, v28.4s, v17.4s\n"
+        "fmin v31.4s, v31.4s, v16.4s\n"
+        "fmin v30.4s, v30.4s, v16.4s\n"
+        "fmin v29.4s, v29.4s, v16.4s\n"
+        "fmin v28.4s, v28.4s, v16.4s\n"
+        "fcvtn v31.4h, v31.4s\n"
+        "fcvtn v30.4h, v30.4s\n"
+        "fcvtn v29.4h, v29.4s\n"
+        "fcvtn v28.4h, v28.4s\n"
+        "blt 19f\n"
+        "mov x20, %x[dst]\n"
+        "cmp x13, #0x1\n"
+        "str d31, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "ble 22f\n"
+        "cmp x13, #0x2\n"
+        "str d30, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "ble 22f\n"
+        "cmp x13, #0x3\n"
+        "str d29, [x20, #0x0]\n"
+        "add x20, x20, %x[dst_stride_row]\n"
+        "ble 22f\n"
+        "str d28, [x20, #0x0]\n"
+        "b 22f\n"
+        "19:"  // Row tail: Partial output
+        "mov x23, %x[dst]\n"
+        "cmp x13, #0x1\n"
+        "add x22, x23, %x[dst_stride_row]\n"
+        "csel x22, x22, x23, GT\n"
+        "cmp x13, #0x2\n"
+        "add x21, x23, %x[dst_stride_row], LSL #1\n"
+        "csel x21, x21, x22, GT\n"
+        "cmp x13, #0x3\n"
+        "add x20, x21, %x[dst_stride_row]\n"
+        "csel x20, x20, x21, GT\n"
+        "tbz x25, #1, 20f\n"
+        "st1 { v28.s }[0], [x20], #0x4\n"
+        "st1 { v29.s }[0], [x21], #0x4\n"
+        "st1 { v30.s }[0], [x22], #0x4\n"
+        "st1 { v31.s }[0], [x23], #0x4\n"
+        "tbz x25, #0, 21f\n"
+        "st1 { v28.h }[2], [x20]\n"
+        "st1 { v29.h }[2], [x21]\n"
+        "st1 { v30.h }[2], [x22]\n"
+        "st1 { v31.h }[2], [x23]\n"
+        "b 21f\n"
+        "20:"  // Row tail: Output block 0: partial_1_0
+        "st1 { v28.h }[0], [x20]\n"
+        "st1 { v29.h }[0], [x21]\n"
+        "st1 { v30.h }[0], [x22]\n"
+        "st1 { v31.h }[0], [x23]\n"
+        "21:"  // Row tail: Output block 0: Done
+        "22:"  // Row tail: Output stage exit
+        "subs x25, x25, #0x4\n"
+        "add %x[dst], %x[dst], #0x08\n"
+        "bgt 16b\n"
+        "subs x13, x13, #0x4\n"
+        "add %x[lhs_packed], %x[lhs_packed], x12\n"
+        "mov %x[dst], x24\n"
+        "bgt 15b\n"
+        "23:"  // Row tail: Row loop skip
+        : [dst] "+&r"(dst), [lhs_packed] "+&r"(lhs_packed)
+        : [clamp_vals] "r"(clamp_vals), [dst_stride_row] "r"(dst_stride_row), [m] "r"(m), [n] "r"(n),
+          [num_blocks] "r"(num_blocks), [rhs_packed] "r"(rhs_packed)
+        : "cc", "memory", "v0", "v1", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v2", "v20",
+          "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v3", "v30", "v31", "v4", "v5", "v6", "v7",
+          "v8", "v9", "x10", "x11", "x12", "x13", "x20", "x21", "x22", "x23", "x24", "x25", "x26", "x27", "x28", "x9");
+}
+
+#endif  // Architectural feature check
diff --git a/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h
new file mode 100644
index 0000000..69ed9b1
--- /dev/null
+++ b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.h
@@ -0,0 +1,144 @@
+
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#pragma once
+
+#include <stddef.h>
+#include "kai/kai_common.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/// Micro-kernel dependencies
+///
+/// -# kai_lhs_quant_pack_qsi8d32p_f16 to dynamically quantize and pack the LHS matrix
+/// -# kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0 to pack the RHS matrix
+
+/// --------------------------------------------------
+
+/// Gets the m step value.
+/// The micro-kernel can process any M values. However, the starting M index to
+/// be processed must be a multiple of m step.
+///
+/// @return the m step value
+size_t kai_get_m_step_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the n step value.
+/// The micro-kernel can process any N values. However, the starting N index to
+/// be processed must be a multiple of n step.
+///
+/// @return the n step
+size_t kai_get_n_step_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the mr value, which must be used to pack the LHS matrix
+///
+/// @return the mr value
+size_t kai_get_mr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the nr value, which must be used to pack the RHS matrix.
+///
+/// @return the nr value
+size_t kai_get_nr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the kr value, which must be used to pack the LHS and RHS matrices
+///
+/// @return the kr value
+size_t kai_get_kr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the sr value, which must be used to pack the LHS and RHS matrices
+///
+/// @return the sr value
+size_t kai_get_sr_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(void);
+
+/// Gets the offset in bytes for the packed LHS matrix,
+/// which contains the packed Signed 8-bit quantized symmetric per-block (qsi8d32) values.
+///
+/// This function should be called before passing the pointer to the packed LHS matrix to the micro-kernel.
+///
+/// @param[in] m_idx Row index in the LHS matrix (not packed). It must be a multiple of 16.
+/// @param[in] k     Total number of columns in the LHS matrix (not packed).
+/// @param[in] bl    Block length. It must be 32.
+///
+/// @return the offset in bytes to the packed LHS matrix
+size_t kai_get_lhs_packed_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m_idx,  //
+    size_t k,      //
+    size_t bl);    //
+
+/// Gets the offset in bytes for the packed RHS matrix,
+/// which contains the packed Signed 4-bit quantized symmetric per-block (qsi4c32) values.
+///
+/// @param[in] n_idx Row index in the RHS matrix (not packed). It must be a multiple of 4.
+/// @param[in] k     The common dimension between the LHS and RHS matrix (K).
+/// @param[in] bl    Block length. It must be 32.
+///
+/// @return the offset in bytes to the packed RHS matrix
+size_t kai_get_rhs_packed_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t n_idx,  //
+    size_t k,      //
+    size_t bl);    //
+
+/// Gets the offset in bytes for the DST matrix
+///
+/// @param[in] m_idx      Row index in the DST matrix. It must be a multiple of 16.
+/// @param[in] n_idx      Column index in the DST matrix. It must be multiple of 4.
+/// @param[in] dst_stride The number of bytes in in each row of the DST matrix
+///
+/// @return the DST offset in bytes
+size_t kai_get_dst_offset_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m_idx,        //
+    size_t n_idx,        //
+    size_t dst_stride);  //
+
+/// Gets the size in bytes for the destination (DST) matrix.
+///
+/// @param[in] m Number of rows in the destination (DST) matrix.
+/// @param[in] n Number of columns in the destination (DST) matrix.
+///
+/// @return the destination (DST) matrix size in bytes
+size_t kai_get_dst_size_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m,   //
+    size_t n);  //
+
+/// Runs the matrix multiplication (matmul) micro-kernel followed by a clamp (min-max) operation.
+///
+/// LHS matrix: Signed 8-bit quantized symmetric per-block (qsi8d32) and packed
+/// RHS matrix: Signed 4-bit quantized symmetric per-block (qsi4c32) and packed.
+/// Output tile: (rows x cols) = 16 x 4
+/// Accumulation performed in a single for loop: 32
+/// Extension used: i8mm
+///
+/// @param[in]  m              The number of output rows written.
+/// @param[in]  n              The number of output columns written.
+/// @param[in]  k              The number of channels. The common dimension between the LHS and RHS matrix.
+/// @param[in]  bl             Block length. It must be 32.
+/// @param[in]  lhs_packed     The LHS packed matrix.
+///                            When the activation are dynamically quantized, you can obtain this matrix
+///                            by calling the @ref kai_lhs_quant_pack_qsi8d32p_f16 micro-kernel which performs
+///                            both the dynamic quantization to 8-bit and activation packing in a single step.
+/// @param[in]  rhs_packed     The RHS packed matrix, which is obtained by calling @ref
+/// kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0
+/// @param[out] dst            The DST matrix.
+/// @param[in]  dst_stride_row Stride in bytes between two rows of the DST matrix.
+/// @param[in]  dst_stride_col Stride in bytes between two columns of the DST matrix. It must be sizeof(float16_t).
+/// @param[in]  scalar_min     Min value used to clamp the final result.
+/// @param[in]  scalar_max     Max value used to clamp the final result.
+void kai_run_matmul_clamp_f16_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm(
+    size_t m,                //
+    size_t n,                //
+    size_t k,                //
+    size_t bl,               //
+    const void* lhs_packed,  //
+    const void* rhs_packed,  //
+    float16_t* dst,          //
+    size_t dst_stride_row,   //
+    size_t dst_stride_col,   //
+    float scalar_min,        //
+    float scalar_max);       //
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_interface.h b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_interface.h
new file mode 100644
index 0000000..ae4d162
--- /dev/null
+++ b/kai/ukernels/matmul/matmul_clamp_f16_qsi8d32p_qsi4c32p/kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_interface.h
@@ -0,0 +1,55 @@
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#pragma once
+
+#include <stddef.h>
+#include <stdint.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+// All micro-kernels variants of the same type share the same interfaces
+// In this case, the micro-kernel type is: matmul_clamp_f16_qsi8d32p_qsi4c32p
+
+/// Micro-kernel helper functions ("get" methods)
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_m_step_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_n_step_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_mr_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_nr_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_kr_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_sr_func_t)(void);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_lhs_packed_offset_func_t)(
+    size_t m_idx, size_t k, size_t bl);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_rhs_packed_offset_func_t)(
+    size_t n_idx, size_t k, size_t bl);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_dst_offset_func_t)(
+    size_t m_idx, size_t n_idx, size_t dst_stride);
+typedef size_t (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_dst_size_func_t)(size_t m, size_t n);
+
+/// Micro-kernel core function ("run" method)
+typedef void (*kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_run_matmul_func_t)(
+    size_t m, size_t n, size_t k, size_t bl, const void* lhs_p, const void* rhs_p, float16_t* dst, size_t dst_stride_row,
+    size_t dst_stride_col, float scalar_min, float scalar_max);
+
+/// Micro-kernel interface
+struct kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_ukernel {
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_m_step_func_t get_m_step;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_n_step_func_t get_n_step;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_mr_func_t get_mr;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_nr_func_t get_nr;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_kr_func_t get_kr;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_sr_func_t get_sr;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_lhs_packed_offset_func_t get_lhs_packed_offset;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_rhs_packed_offset_func_t get_rhs_packed_offset;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_dst_offset_func_t get_dst_offset;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_get_dst_size_func_t get_dst_size;
+    kai_matmul_clamp_f16_qsi8d32p_qsi4c32p_run_matmul_func_t run_matmul;
+};
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.c b/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.c
new file mode 100644
index 0000000..208c512
--- /dev/null
+++ b/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.c
@@ -0,0 +1,124 @@
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#include "kai_lhs_quant_pack_qsi8d32p_f32.h"
+
+#include <math.h>
+#include <stdint.h>
+
+#include "kai/kai_common.h"
+
+static const size_t kai_num_bytes_multiplier = sizeof(uint16_t);
+
+inline static size_t kai_num_bytes_per_block(size_t bl) {
+    return bl * sizeof(int8_t) + kai_num_bytes_multiplier;
+}
+
+inline static size_t kai_num_blocks_per_row(size_t k, size_t bl) {
+    KAI_ASSERT((k % bl) == 0);
+    return k / bl;
+}
+
+inline static size_t kai_lhs_packed_stride(size_t k, size_t mr, size_t kr, size_t bl) {
+    KAI_UNUSED(kr);
+    return mr * kai_num_blocks_per_row(k, bl) * kai_num_bytes_per_block(bl);
+}
+
+size_t kai_get_m_step_lhs_quant_pack_qsi8d32p_f16(size_t mr) {
+    KAI_UNUSED(mr);
+    return 1;
+}
+
+size_t kai_get_lhs_offset_lhs_quant_pack_qsi8d32p_f16(size_t m_idx, size_t lhs_stride) {
+    return m_idx * lhs_stride;
+}
+
+size_t kai_get_lhs_packed_offset_lhs_quant_pack_qsi8d32p_f16(
+    size_t m_idx, size_t k, size_t bl, size_t mr, size_t kr, size_t sr) {
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+
+    KAI_UNUSED(sr);
+    KAI_UNUSED(kr);
+
+    return (m_idx / mr) * kai_lhs_packed_stride(k, mr, kr, bl);
+}
+
+size_t kai_get_lhs_packed_size_lhs_quant_pack_qsi8d32p_f16(
+    size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr) {
+    KAI_ASSUME((k % 2) == 0);
+    KAI_ASSUME((k % kr) == 0);
+    KAI_ASSUME((k % bl) == 0);
+
+    KAI_UNUSED(sr);
+    KAI_UNUSED(kr);
+
+    const size_t num_rows = kai_roundup(m, mr) / mr;
+
+    return num_rows * kai_lhs_packed_stride(k, mr, kr, bl);
+}
+
+void kai_run_lhs_quant_pack_qsi8d32p_f16(
+    size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr, size_t m_idx_start, const float16_t* lhs,
+    size_t lhs_stride, void* lhs_packed) {
+    if (m == 0) {
+        return;
+    }
+
+    const size_t num_rows = m;
+    const size_t k_block_len = kr / sr;
+    const size_t lhs_packed_stride = kai_lhs_packed_stride(k, mr, kr, bl);
+    const size_t num_blocks_per_row = kai_num_blocks_per_row(k, bl);
+    const size_t num_bytes_per_block = kai_num_bytes_per_block(bl);
+
+    for (size_t row_idx = 0; row_idx < num_rows; ++row_idx) {
+        const float16_t* src_ptr = (const float16_t*)((const uint8_t*)lhs + (row_idx + m_idx_start) * lhs_stride);
+
+        for (size_t b = 0; b < num_blocks_per_row; ++b) {
+            float abs_max = 0.0F;
+
+            const size_t dst_x = ((row_idx + m_idx_start) % mr);
+            int8_t* dst_ptr = (int8_t*)lhs_packed + (b * mr) * num_bytes_per_block;
+
+            for (size_t idx_v = 0; idx_v < bl; ++idx_v) {
+                const float val = src_ptr[idx_v];
+                abs_max = KAI_MAX(abs_max, fabsf(val));
+            }
+
+            // Calculate scale and reciprocal
+            const float scale = abs_max / ((1 << 7) - 1);
+            const float rep_scale = scale ? 1.0F / scale : 0.0F;
+
+            *((uint16_t*)(dst_ptr + dst_x * kai_num_bytes_multiplier)) = kai_cast_f16_f32(scale);
+            dst_ptr += mr * kai_num_bytes_multiplier;
+
+            dst_ptr += dst_x * k_block_len * sizeof(int8_t);
+
+            // Quantize and pack the block
+            for (size_t k_idx = 0; k_idx < bl; k_idx += k_block_len) {
+                for (size_t k_block_idx = 0; k_block_idx < k_block_len; ++k_block_idx) {
+                    // Clamp at the last valid k-index
+                    const size_t k_idx_start = KAI_MIN(k_idx + k_block_idx, k - 1);
+
+                    const float src0_0 = *(src_ptr + k_idx_start);
+
+                    // Scale the values
+                    int32_t v0_s32 = (int32_t)(roundf(src0_0 * rep_scale));
+
+                    *dst_ptr = (int8_t)v0_s32;
+                    dst_ptr += sizeof(int8_t);
+                }
+                dst_ptr += (mr - 1) * k_block_len * sizeof(int8_t);
+            }
+
+            src_ptr += bl;
+        }
+        // Move to the next row if we have interleaved all Mr rows
+        if ((((row_idx + 1) + m_idx_start) % mr) == 0) {
+            lhs_packed = (void*)((int8_t*)lhs_packed + lhs_packed_stride);
+        }
+    }
+}
diff --git a/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.h b/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.h
new file mode 100644
index 0000000..33004c6
--- /dev/null
+++ b/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f16.h
@@ -0,0 +1,84 @@
+//
+// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
+//
+// SPDX-License-Identifier: Apache-2.0
+//
+#pragma once
+
+#include <stddef.h>
+#include <stdint.h>
+#include "kai/kai_common.h"
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/// Gets the m step value.
+/// The micro-kernel can process any M values. However, the starting M index to
+/// be processed must be a multiple of m step.
+///
+/// @param[in] mr The number of M rows to interleave on the same output row.
+///
+/// @return the m step value
+size_t kai_get_m_step_lhs_quant_pack_qsi8d32p_f16(size_t mr);
+
+/// Gets the offset in bytes for the LHS matrix (not packed)
+///
+/// This function should be called before passing the pointer to the LHS matrix to the micro-kernel.
+///
+/// @param[in] m_idx      Row index in the LHS matrix (not packed).
+/// @param[in] lhs_stride The number of bytes in in each row of the LHS matrix (not packed)
+///
+/// @return the offset in bytes to the LHS matrix
+size_t kai_get_lhs_offset_lhs_quant_pack_qsi8d32p_f16(size_t m_idx, size_t lhs_stride);
+
+/// Gets the offset in bytes for the packed LHS matrix,
+/// which contains the packed 8-bit quantized symmetric per-block (qsi8d32) values.
+///
+/// This function should be called before passing the pointer to the packed LHS matrix to the micro-kernel.
+///
+/// @param[in] m_idx Row index in the LHS matrix (not packed).
+/// @param[in] k     Total number of columns in the LHS matrix (not packed).
+/// @param[in] bl    The block length.
+/// @param[in] mr    The number of M rows to interleave on the same output row.
+/// @param[in] kr    The number of columns loaded in the single inner most loop of the matmul micro-kernel.
+/// @param[in] sr    The number of kr splits. It can be 1 (no splits) up to kr.
+///
+/// @return the offset in bytes to the packed LHS matrix
+size_t kai_get_lhs_packed_offset_lhs_quant_pack_qsi8d32p_f16(
+    size_t m_idx, size_t k, size_t bl, size_t mr, size_t kr, size_t sr);
+
+/// Gets the size in bytes for the quantized and packed LHS matrix
+///
+/// @param[in] m  Total number of rows in the LHS matrix (not packed).
+/// @param[in] k  Total number of columns in the LHS matrix (not packed).
+/// @param[in] bl The block length, which defines the number of K values stored in a single block. It must be a
+/// multiple of 32.
+/// @param[in] mr The number of M rows to interleave on the same output row.
+/// @param[in] kr The number of columns loaded in the single inner most loop of the matmul micro-kernel.
+/// @param[in] sr The number of kr splits. It can be 1 (no splits) up to kr.
+///
+/// @return the packed LHS matrix size in bytes
+size_t kai_get_lhs_packed_size_lhs_quant_pack_qsi8d32p_f16(
+    size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr);
+
+/// Run the micro-kernel to quantize and pack the LHS matrix.
+///
+/// @param[in]  m           The number of output rows written.
+/// @param[in]  k           The number of channels. The common dimension of LHS & RHS. It must be multiple of 8.
+/// @param[in]  bl          The block length, which defines the number of K values stored in a single block. It must be
+/// a multiple of 32.
+/// @param[in]  mr          The number of M rows to interleave on the same output row.
+/// @param[in]  kr          The number of columns loaded in the single inner most loop of the matmul micro-kernel.
+/// @param[in]  sr          The number of kr splits. It can be 1 (no splits) up to kr.
+///                         However, kr must be multiple of sr.
+/// @param[in]  m_idx_start The starting M index.
+/// @param[in]  lhs         LHS matrix.
+/// @param[in]  lhs_stride  Stride in bytes between two rows of LHS.
+/// @param[out] lhs_packed  The quantized and packed LHS matrix.
+void kai_run_lhs_quant_pack_qsi8d32p_f16(
+    size_t m, size_t k, size_t bl, size_t mr, size_t kr, size_t sr, size_t m_idx_start, const float16_t* lhs,
+    size_t lhs_stride, void* lhs_packed);
+
+#ifdef __cplusplus
+}
+#endif
--
2.34.1

