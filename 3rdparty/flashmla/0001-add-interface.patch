diff --git csrc/flashmla.h csrc/flashmla.h
new file mode 100644
index 0000000..b3daa1c
--- /dev/null
+++ csrc/flashmla.h
@@ -0,0 +1,42 @@
+#pragma once
+
+#include <torch/torch.h>
+#include <optional>
+#include <vector>
+
+std::vector<at::Tensor>
+get_mla_metadata(
+    at::Tensor &seqlens_k,
+    const int num_heads_per_head_k,
+    const int num_heads_k
+);
+
+std::vector<at::Tensor>
+mha_fwd_kvcache_mla(
+    at::Tensor &q,                               // batch_size x seqlen_q x num_heads x head_size
+    const at::Tensor &kcache,                    // num_blocks x page_block_size x num_heads_k x head_size
+    std::optional<const at::Tensor> &vcache_,    // num_blocks x page_block_size x num_heads_k x head_size_v
+    const int head_size_v,
+    const at::Tensor &seqlens_k,                 // batch_size
+    const at::Tensor &block_table,               // batch_size x max_num_blocks_per_seq
+    const float softmax_scale,
+    bool is_causal,
+    const at::Tensor &tile_scheduler_metadata,   // num_sm_parts x TileSchedulerMetaDataSize
+    const at::Tensor &num_splits                 // batch_size + 1
+);
+
+inline std::vector<at::Tensor>
+mha_fwd_unified_kvcache_mla(
+    at::Tensor &q,                               // batch_size x seqlen_q x num_heads x head_size
+    const at::Tensor &kvcache,                   // num_blocks x page_block_size x num_heads_k x head_size
+    const int head_size_v,
+    const at::Tensor &seqlens_k,                 // batch_size
+    const at::Tensor &block_table,               // batch_size x max_num_blocks_per_seq
+    const float softmax_scale,
+    bool is_causal,
+    const at::Tensor &tile_scheduler_metadata,   // num_sm_parts x TileSchedulerMetaDataSize
+    const at::Tensor &num_splits                 // batch_size + 1
+) {
+    std::optional<const at::Tensor> vcache = std::nullopt;
+    return mha_fwd_kvcache_mla(q, kvcache, vcache, head_size_v, seqlens_k, block_table, softmax_scale, is_causal, tile_scheduler_metadata, num_splits);
+}
