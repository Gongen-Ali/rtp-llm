From d31433d3cb1467a1a5a1e70f3a7ab62d114cf79c Mon Sep 17 00:00:00 2001
From: "baowending.bwd" <baowending.bwd@alibaba-inc.com>
Date: Thu, 20 Feb 2025 22:53:45 +0800
Subject: [PATCH] update - add mla attn test && impl mla write kvcache


diff --git a/csrc/flashinfer_ops.cu b/csrc/flashinfer_ops.cu
index b6c2f7c..e7161aa 100644
--- csrc/flashinfer_ops.cu
+++ csrc/flashinfer_ops.cu
@@ -86,6 +86,11 @@ void append_paged_kv_cache(at::Tensor append_key, at::Tensor append_value, at::T
                            at::Tensor kv_indices, at::Tensor kv_indptr, at::Tensor kv_last_page_len,
                            int64_t layout, int64_t cuda_stream);
 
+void append_paged_mla_kv_cache(at::Tensor append_ckv, at::Tensor append_kpe, at::Tensor batch_indices,
+                           at::Tensor positions, at::Tensor ckv_cache, at::Tensor kpe_cache,
+                           at::Tensor kv_indices, at::Tensor kv_indptr, at::Tensor kv_last_page_len,
+                           int64_t cuda_stream);
+
 void block_sparse_indices_to_vector_sparse_offsets(at::Tensor block_sparse_indices,
                                                    at::Tensor block_sparse_indptr,
                                                    at::Tensor vector_sparse_offsets,
@@ -242,6 +247,8 @@ TORCH_LIBRARY_FRAGMENT(TORCH_EXTENSION_NAME, m) {
   // page
   // Append paged KV-Cache operator
   m.def("append_paged_kv_cache", append_paged_kv_cache);
+  // Append paged MLA KV-Cache operator
+  m.def("append_paged_mla_kv_cache", append_paged_mla_kv_cache);
   // Precompute block sparse offsets
   m.def("block_sparse_indices_to_vector_sparse_offsets",
         block_sparse_indices_to_vector_sparse_offsets);
diff --git a/csrc/flashinfer_page_ops.cu b/csrc/flashinfer_page_ops.cu
index 349100d..0cc6a2a 100644
--- csrc/flashinfer_page_ops.cu
+++ csrc/flashinfer_page_ops.cu
@@ -20,6 +20,12 @@ void append_paged_kv_cache(at::Tensor append_key, at::Tensor append_value, at::T
                            at::Tensor kv_indices, at::Tensor kv_indptr, at::Tensor kv_last_page_len,
                            int64_t layout, int64_t cuda_stream);
 
+void append_paged_mla_kv_cache(at::Tensor append_ckv, at::Tensor append_kpe, at::Tensor batch_indices,
+                           at::Tensor positions, at::Tensor ckv_cache, at::Tensor kpe_cache,
+                           at::Tensor kv_indices, at::Tensor kv_indptr, at::Tensor kv_last_page_len,
+                           int64_t cuda_stream);
+
+
 void block_sparse_indices_to_vector_sparse_offsets(at::Tensor block_sparse_indices,
                                                    at::Tensor block_sparse_indptr,
                                                    at::Tensor vector_sparse_offsets,
@@ -31,6 +37,8 @@ void block_sparse_indices_to_vector_sparse_offsets(at::Tensor block_sparse_indic
 TORCH_LIBRARY_FRAGMENT(TORCH_EXTENSION_NAME, m) {
   // "Append paged KV-Cache operator"
   m.def("append_paged_kv_cache", append_paged_kv_cache);
+  // "Append paged MLA KV-Cache operator"
+  m.def("append_paged_mla_kv_cache", append_paged_mla_kv_cache);
   // "Precompute block sparse offsets"
   m.def("block_sparse_indices_to_vector_sparse_offsets",
         block_sparse_indices_to_vector_sparse_offsets);
diff --git a/csrc/page.cu b/csrc/page.cu
index dbc8d6c..0ad4ade 100644
--- csrc/page.cu
+++ csrc/page.cu
@@ -137,3 +137,82 @@ void block_sparse_indices_to_vector_sparse_offsets(at::Tensor block_sparse_indic
   TORCH_CHECK(status == cudaSuccess, "BlockSparseIndicesToVectorSparseOffset failed with error: ",
               cudaGetErrorString(status));
 }
+
+void append_paged_mla_kv_cache(at::Tensor append_ckv, at::Tensor append_kpe, at::Tensor batch_indices,
+                           at::Tensor positions, at::Tensor ckv_cache, at::Tensor kpe_cache,
+                           at::Tensor kv_indices, at::Tensor kv_indptr, at::Tensor kv_last_page_len,
+                           int64_t cuda_stream) {
+  CHECK_LAST_DIM_CONTIGUOUS(append_ckv);
+  CHECK_LAST_DIM_CONTIGUOUS(append_kpe);
+  CHECK_INPUT(batch_indices);
+  CHECK_INPUT(positions);
+  // NOTE(Zihao): doesn't have to be contiguous
+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(ckv_cache);
+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(kpe_cache);
+  CHECK_INPUT(kv_indices);
+  CHECK_INPUT(kv_indptr);
+  CHECK_INPUT(kv_last_page_len);
+  CHECK_DIM(2, append_ckv);
+  CHECK_DIM(2, append_kpe);
+  CHECK_DIM(1, batch_indices);
+  CHECK_DIM(1, positions);
+  CHECK_DIM(3, ckv_cache);
+  CHECK_DIM(3, kpe_cache);
+  CHECK_DIM(1, kv_indices);
+  CHECK_DIM(1, kv_indptr);
+  CHECK_DIM(1, kv_last_page_len);
+  unsigned int nnz = append_ckv.size(0);
+  unsigned int batch_size = kv_last_page_len.size(0);
+  CHECK_EQ(kv_indptr.size(0), batch_size + 1);
+  CHECK_EQ(batch_indices.size(0), nnz);
+  CHECK_EQ(positions.size(0), nnz);
+  auto device = append_ckv.device();
+  CHECK_EQ(append_ckv.device(), device);
+  CHECK_EQ(append_kpe.device(), device);
+  CHECK_EQ(ckv_cache.device(), device);
+  
+  CHECK_EQ(kv_indices.device(), device);
+  CHECK_EQ(kv_indptr.device(), device);
+  CHECK_EQ(kv_last_page_len.device(), device);
+
+  unsigned int page_size, ckv_dim, kpe_dim;
+  page_size = ckv_cache.size(1);
+  ckv_dim = ckv_cache.size(2);
+  kpe_dim = kpe_cache.size(2);
+
+  // get kv_cache_strides
+  const int64_t* ckv_strides = ckv_cache.strides().data();
+  const int64_t* kpe_strides = kpe_cache.strides().data();
+ 
+  auto append_ckv_strides = append_ckv.strides();
+  auto append_ckv_stride_n = append_ckv_strides[0];
+  auto append_kpe_strides = append_kpe.strides();
+  auto append_kpe_stride_n = append_kpe_strides[0];
+
+  CHECK_EQ(append_ckv.size(1), ckv_dim);
+  CHECK_EQ(append_kpe.size(1), kpe_dim);
+
+  auto kv_scalar_dtype = ckv_cache.scalar_type();
+
+  cudaStream_t stream = reinterpret_cast<cudaStream_t>(cuda_stream);
+  bool success = DISPATCH_PYTORCH_DTYPE_TO_CTYPE(kv_scalar_dtype, c_type, [&] {
+    paged_kv_mla_t<c_type, int32_t> paged_mla_kv(
+        page_size, ckv_dim, kpe_dim, batch_size, 
+        static_cast<c_type*>(ckv_cache.data_ptr()),
+        ckv_strides,
+        static_cast<c_type*>(kpe_cache.data_ptr()), kpe_strides,
+        static_cast<int32_t*>(kv_indices.data_ptr()), static_cast<int32_t*>(kv_indptr.data_ptr()),
+        static_cast<int32_t*>(kv_last_page_len.data_ptr()));
+    cudaError_t status =
+        AppendPagedKVMlaCache(paged_mla_kv, static_cast<c_type*>(append_ckv.data_ptr()),
+                           static_cast<c_type*>(append_kpe.data_ptr()),
+                           static_cast<int32_t*>(batch_indices.data_ptr()),
+                           static_cast<int32_t*>(positions.data_ptr()), nnz, append_ckv_stride_n,
+                           append_kpe_stride_n, stream);
+    TORCH_CHECK(status == cudaSuccess,
+                "AppendPagedKVMlaCache failed with error: ", cudaGetErrorString(status));
+    return true;
+  });
+
+  TORCH_CHECK(success, "AppendPagedKVMlaCache failed to dispatch with dtype ", kv_scalar_dtype);
+}
\ No newline at end of file
diff --git a/flashinfer/__init__.py b/flashinfer/__init__.py
index ba2d081..83e0d52 100644
--- flashinfer/__init__.py
+++ flashinfer/__init__.py
@@ -45,6 +45,7 @@ from .norm import gemma_fused_add_rmsnorm as gemma_fused_add_rmsnorm
 from .norm import gemma_rmsnorm as gemma_rmsnorm
 from .norm import rmsnorm as rmsnorm
 from .page import append_paged_kv_cache as append_paged_kv_cache
+from .page import _append_paged_mla_kv_cache_kernel as _append_paged_mla_kv_cache_kernel
 from .page import get_batch_indices_positions as get_batch_indices_positions
 from .page import get_seq_lens as get_seq_lens
 from .prefill import (
diff --git a/flashinfer/page.py b/flashinfer/page.py
index 898dafd..a10d129 100644
--- flashinfer/page.py
+++ flashinfer/page.py
@@ -88,6 +88,39 @@ def block_sparse_indices_to_vector_sparse_offsets(
         )
     return vector_sparse_offsets
 
+@register_custom_op(
+    "flashinfer::append_paged_mla_kv_cache",
+    mutates_args=("ckv_cache", "kpe_cache"),
+)
+def _append_paged_mla_kv_cache_kernel(
+    append_ckv: torch.Tensor,
+    append_kpe: torch.Tensor,
+    batch_indices: torch.Tensor,
+    positions: torch.Tensor,
+    ckv_cache: Optional[torch.Tensor],
+    kpe_cache: Optional[torch.Tensor],
+    kv_indices: torch.Tensor,
+    kv_indptr: torch.Tensor,
+    kv_last_page_len: torch.Tensor,
+) -> None:
+    with append_ckv.device as device:
+        batch_indices = batch_indices.int()
+        positions = positions.int()
+        kv_indices = kv_indices.int()
+        kv_indptr = kv_indptr.int()
+        kv_last_page_len = kv_last_page_len.int()        
+        get_page_module().append_paged_mla_kv_cache(
+            append_ckv,
+            append_kpe,
+            batch_indices,
+            positions,
+            ckv_cache,
+            kpe_cache,
+            kv_indices,
+            kv_indptr,
+            kv_last_page_len,
+            get_cuda_stream(device)
+        )
 
 @register_custom_op(
     "flashinfer::append_paged_kv_cache",
diff --git a/include/flashinfer/page.cuh b/include/flashinfer/page.cuh
index ce8aeb9..c46b184 100644
--- include/flashinfer/page.cuh
+++ include/flashinfer/page.cuh
@@ -19,6 +19,7 @@
 #include <driver_types.h>
 
 #include <vector>
+#include <assert.h>
 
 #include "fastdiv.cuh"
 #include "layout.cuh"
@@ -559,8 +560,85 @@ struct paged_kv_mla_t {
       return 0;
     }
   }
+
+  __device__ __forceinline__ DType* get_ckv_ptr(size_t page_idx, size_t entry_idx,
+                                                          size_t feat_idx) const {
+    return ckv_data + get_elem_offset_ckv(__ldg(indices + page_idx), entry_idx, feat_idx);
+  }
+
+  __device__ __forceinline__ DType* get_kpe_ptr(size_t page_idx, size_t entry_idx,
+                                                          size_t feat_idx) const {
+    return kpe_data + get_elem_offset_kpe(__ldg(indices + page_idx), entry_idx, feat_idx);
+  }
 };
 
+template <uint32_t head_dim_ckv, uint32_t head_dim_kpe, uint32_t vec_size, typename DType, typename IdType>
+__global__ void AppendPagedKVMlaCacheKernel(paged_kv_mla_t<DType, IdType> paged_kv_mla,
+                                            DType* __restrict__ append_ckv,
+                                            DType* __restrict__ append_kpe,
+                                            IdType* __restrict__ batch_indices,
+                                            IdType* __restrict__ positions,
+                                            uint32_t nnz,
+                                            size_t append_ckv_stride_n,
+                                            size_t append_kpe_stride_n) {
+  uint32_t tx = threadIdx.x;
+  uint32_t cta_id = blockIdx.x;
+  uint32_t num_ctas = gridDim.x;
+
+#pragma unroll 4
+  for (uint32_t i = cta_id; i < nnz; i += num_ctas) {
+    uint32_t page_iter, entry_idx;
+    paged_kv_mla.page_size.divmod(paged_kv_mla.indptr[batch_indices[i]] * paged_kv_mla.page_size + positions[i],
+                              page_iter, entry_idx);
+    DType* ckv_ptr = paged_kv_mla.get_ckv_ptr(page_iter, entry_idx, tx * vec_size);
+    vec_t<DType, vec_size>::memcpy(
+        ckv_ptr, append_ckv + i * append_ckv_stride_n + tx * vec_size);
+
+    if (tx * vec_size < head_dim_kpe) {
+      DType* kpe_ptr = paged_kv_mla.get_kpe_ptr(page_iter, entry_idx, tx * vec_size);
+      vec_t<DType, vec_size>::memcpy(
+        kpe_ptr, append_kpe + i * append_kpe_stride_n + tx * vec_size);
+      }
+  }
+}
+
+
+template <typename DType, typename IdType>
+cudaError_t AppendPagedKVMlaCache(paged_kv_mla_t<DType, IdType> paged_kv, DType* append_ckv,
+                               DType* append_kpe, IdType* batch_indices, IdType* positions,
+                               uint32_t nnz, size_t append_ckv_stride_n, size_t append_kpe_stride_n,
+                               cudaStream_t stream = nullptr) {
+  int dev_id = 0;
+  int num_sms = 0;
+  int num_blocks_per_sm = 0;
+  FLASHINFER_CUDA_CALL(cudaGetDevice(&dev_id));
+  FLASHINFER_CUDA_CALL(cudaDeviceGetAttribute(&num_sms, cudaDevAttrMultiProcessorCount, dev_id));
+  
+  uint32_t head_dim_ckv = paged_kv.head_dim_ckv;
+  uint32_t head_dim_kpe = paged_kv.head_dim_kpe;
+  constexpr uint32_t HEAD_CKV_DIM = 512;
+  constexpr uint32_t HEAD_KPE_DIM = 64;
+  assert(head_dim_ckv == HEAD_CKV_DIM);
+  assert(head_dim_kpe == HEAD_KPE_DIM);
+  constexpr uint32_t vec_size = 2;
+  
+  uint32_t bdx = HEAD_CKV_DIM / vec_size;
+  uint32_t num_threads = bdx;
+  uint32_t smem_size = 0;
+  auto kernel = AppendPagedKVMlaCacheKernel<HEAD_CKV_DIM, HEAD_KPE_DIM, vec_size, DType, IdType>;
+  FLASHINFER_CUDA_CALL(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, kernel,
+                                                                   num_threads, smem_size));
+  num_blocks_per_sm = min(num_blocks_per_sm, ceil_div(int(nnz), num_sms));                                                                   
+  dim3 nblks(num_blocks_per_sm * num_sms);
+  dim3 nthrs(bdx);
+  void* args[] = {(void*)&paged_kv, (void*)&append_ckv, (void*)&append_kpe, (void*)&batch_indices,
+                  (void*)&positions, (void*)&nnz, (void*)&append_ckv_stride_n,
+                  (void*)&append_kpe_stride_n};
+  FLASHINFER_CUDA_CALL(cudaLaunchKernel((void*)kernel, nblks, nthrs, args, 0, stream));
+  return cudaSuccess;
+}
+
+
 }  // namespace flashinfer
 
 #endif  // FLAHSINFER_PAGE_CUH_
diff --git a/tests/test_deepseek_mla.py b/tests/test_deepseek_mla.py
index 2c7dd5f..f21bbe6 100644
--- tests/test_deepseek_mla.py
+++ tests/test_deepseek_mla.py
@@ -144,6 +144,113 @@ def test_batch_prefill_with_ragged_kv_cache(
     torch.testing.assert_close(o, o_ref, rtol=1e-3, atol=1e-3)
     torch.testing.assert_close(lse, lse_ref, rtol=1e-3, atol=1e-3)
 
+def create_indptr(lengths, page_size = 1):
+    result = [0]
+    current_sum = 0
+    for length in lengths:
+        current_sum += math.ceil(length / page_size)
+        result.append(current_sum)
+    return result
+
+def test_paged_attention_with_diff_batch_size():
+    torch.manual_seed(42)
+    kv_len = [10, 17]
+    qo_len = [1, 1]
+    batch_size = sum(qo_len)
+    num_heads = 128
+    causal = True
+    page_size = 8
+    backend = "fa2"
+
+    head_dim_ckv = 512
+    head_dim_kpe = 64
+
+    q_nope = torch.randn(
+        batch_size, num_heads, head_dim_ckv, dtype=torch.half, device="cuda"
+    )
+    q_pe = torch.randn(
+        batch_size, num_heads, head_dim_kpe, dtype=torch.half, device="cuda"
+    )
+
+    single_req_size = [math.ceil(x / page_size) for x in kv_len]
+    total_page_num = sum(single_req_size)
+
+    ckv = torch.randn(
+        total_page_num,
+        page_size,
+        head_dim_ckv,
+        dtype=torch.half,
+        device="cuda",
+    )
+    kpe = torch.randn(
+        total_page_num,
+        page_size,
+        head_dim_kpe,
+        dtype=torch.half,
+        device="cuda",
+    )
+    #temp
+    # kv_len = [17]
+    # qo_len = [1]
+    # batch_size = sum(qo_len)
+    # q_nope = q_nope[1:]
+    # q_pe = q_pe[1:]
+    # single_req_size = single_req_size[1: ]
+    # total_page_num = sum(single_req_size)
+
+    # ckv = ckv[2:]
+    # kpe = kpe[2:]
+
+    sm_scale = 1.0 / ((128 + 64) ** 0.5)  # use head dimension before matrix absorption
+    workspace_buffer = torch.empty(128 * 1024 * 1024, dtype=torch.int8).to(0)
+    wrapper = flashinfer.mla.BatchMLAPagedAttentionWrapper(
+        workspace_buffer, backend=backend
+    )
+    kv_indptr = torch.tensor(create_indptr(kv_len, page_size)).to(0).int()
+    q_indptr = torch.tensor(create_indptr(qo_len)).to(0).int()        
+    kv_indices = torch.arange(0, total_page_num).to(0).int()
+    kv_lens = torch.tensor(kv_len, dtype=torch.int32).to(0)
+    wrapper.plan(
+        q_indptr,
+        kv_indptr,
+        kv_indices,
+        kv_lens,
+        num_heads,
+        head_dim_ckv,
+        head_dim_kpe,
+        page_size,
+        causal,
+        sm_scale,
+        q_nope.dtype,
+        ckv.dtype,
+    )
+    o, lse = wrapper.run(q_nope, q_pe, ckv, kpe, return_lse=True)
+    print(o)
+
+    current_page_num = 0
+    current_q_size = 0
+    for index, single_req_size in enumerate(single_req_size):
+        
+        ckv_slice = ckv[current_page_num:current_page_num + single_req_size]
+        kpe_slice = kpe[current_page_num:current_page_num + single_req_size]
+
+        k = (
+            torch.cat([ckv_slice, kpe_slice], dim=-1)
+            .view(-1, 1, head_dim_ckv + head_dim_kpe)
+            .repeat_interleave(num_heads, dim=1)[:kv_len[index]]
+        )
+        v = ckv_slice.view(-1, 1, head_dim_ckv).repeat_interleave(num_heads, dim=1)[:kv_len[index]]
+    
+        q_slice = torch.cat([q_nope[current_q_size:current_q_size + qo_len[index]], q_pe[current_q_size:current_q_size + qo_len[index]]], dim=-1)                
+        o_ref, lse_ref = attention_ref(1, q_slice, k, v, causal, sm_scale)
+        print(o_ref)
+        lse_ref = lse_ref.flatten(0, 1)
+        torch.testing.assert_close(o[current_q_size:current_q_size + qo_len[index]], o_ref, rtol=1e-3, atol=1e-3)
+        torch.testing.assert_close(lse[current_q_size:current_q_size + qo_len[index]], lse_ref, rtol=1e-3, atol=1e-3)        
+        current_q_size += qo_len[index]
+        current_page_num += single_req_size
+
+
 
 @pytest.mark.parametrize("batch_size", [1, 17, 37])
 @pytest.mark.parametrize("kv_len", [17, 33, 96, 97, 114, 514, 1024])
@@ -236,4 +343,5 @@ if __name__ == "__main__":
     # test_batch_mla_page_attention(37, 33, 1, 128, True, 1, "fa2")
     # test_batch_mla_page_attention(33, 2, 1, 1, False, 1, "fa2")
     # test_batch_mla_page_attention(17, 64, 17, 128, False, 1, "fa2")
-    test_batch_mla_page_attention(4, 64, 17, 32, False, 1, "fa2")
+    # test_batch_mla_page_attention(2, 8, 1, 128, True, 8, "fa2")
+    test_paged_attention_with_diff_batch_size()
diff --git a/tests/test_mla_page.py b/tests/test_mla_page.py
new file mode 100644
index 0000000..7f3cee9
--- /dev/null
+++ tests/test_mla_page.py
@@ -0,0 +1,92 @@
+import pytest
+import torch
+
+import flashinfer
+
+
+def test_append_mla_paged_kv_cache():
+    nnz_kv = 100    
+    ckv_dim = 512
+    kpe_dim = 64
+
+    ckv_append = torch.randn(nnz_kv, ckv_dim).half().to(0)
+    kpe_append = torch.randn(nnz_kv, kpe_dim).half().to(0)
+    # 45 + 8 + 25 + 22 = nnz_kv
+    kv_append_length = torch.tensor([45, 8, 25, 22], dtype=torch.int32, device="cuda:0")
+    kv_append_indptr = torch.cat(
+        [torch.zeros(1).int().to(0), torch.cumsum(kv_append_length, dim=0)]
+    ).int()
+
+    max_num_pages = 1000
+    page_size = 16
+    ckv_cache = (
+        torch.zeros(max_num_pages, page_size, ckv_dim).half().to(0)
+    )
+    kpe_cache = (
+        torch.zeros(max_num_pages, page_size, kpe_dim).half().to(0)
+    )
+    num_pages_per_req = torch.tensor([3, 1, 2, 2], dtype=torch.int32, device="cuda:0")
+    kv_page_indptr = torch.cat(
+        [torch.zeros(1).int().to(0), torch.cumsum(num_pages_per_req, dim=0)]
+    ).int()
+    # use first 8 pages in the paged-kv
+    kv_page_indices = torch.arange(8, dtype=torch.int32, device="cuda:0")
+    # 45 = (3 - 1) * 16 + 13
+    # 8 = (1 - 1) * 16 + 8
+    # 25 = (2 - 1) * 16 + 9
+    # 22 = (2 - 1) * 16 + 6
+    kv_last_page_len = torch.tensor([13, 8, 9, 6], dtype=torch.int32, device="cuda:0")
+    batch_indices, positions = flashinfer.get_batch_indices_positions(
+        kv_append_indptr,
+        flashinfer.get_seq_lens(kv_page_indptr, kv_last_page_len, page_size),
+        nnz_kv,
+    )    
+    flashinfer._append_paged_mla_kv_cache_kernel(
+        ckv_append,
+        kpe_append,
+        batch_indices,
+        positions,
+        ckv_cache,
+        kpe_cache,
+        kv_page_indices,
+        kv_page_indptr,
+        kv_last_page_len,
+    ) 
+
+    # 45 + 8 + 25 + 22 = nnz_kv
+    ckv_cache = ckv_cache.view(-1, ckv_dim)
+    kpe_cache = kpe_cache.view(-1, kpe_dim)
+    assert(torch.all(torch.isclose(ckv_append[:45], ckv_cache[:45])))
+    assert(torch.all(torch.isclose(kpe_append[:45], kpe_cache[:45])))
+    assert(bool(torch.all(ckv_cache[45:48] == 0)))
+    assert(bool(torch.all(kpe_cache[45:48] == 0)))
+    
+    assert(torch.all(torch.isclose(kpe_append[45:53], kpe_cache[48:56])))
+    assert(torch.all(torch.isclose(ckv_append[45:53], ckv_cache[48:56])))
+    assert(bool(torch.all(ckv_cache[56:64] == 0)))
+    assert(bool(torch.all(kpe_cache[56:64] == 0)))
+
+    assert(torch.all(torch.isclose(kpe_append[53:78], kpe_cache[64:89])))
+    assert(torch.all(torch.isclose(ckv_append[53:78], ckv_cache[64:89])))
+    assert(bool(torch.all(ckv_cache[89:96] == 0)))
+    assert(bool(torch.all(kpe_cache[89:96] == 0)))
+
+    assert(torch.all(torch.isclose(kpe_append[78:100], kpe_cache[96:118])))
+    assert(torch.all(torch.isclose(ckv_append[78:100], ckv_cache[96:118])))
+    assert(bool(torch.all(ckv_cache[118:] == 0)))
+    assert(bool(torch.all(kpe_cache[118:] == 0)))
+
+    # flashinfer._append_paged_mla_kv_cache_kernel(
+    #     k_append,
+    #     v_append,
+    #     batch_indices,
+    #     positions,
+    #     paged_kv_cache,
+    #     paged_kv_cache,
+    #     kv_page_indices,
+    #     kv_page_indptr,
+    #     kv_last_page_len
+    # )
+
+if __name__ == "__main__":
+    test_append_mla_paged_kv_cache()
\ No newline at end of file
-- 
2.19.1.6.gb485710b

